#!/usr/bin/env python3
"""
Simple script to trigger event scraping and save results
"""

import requests
import json

def trigger_scraping():
    """Trigger the scraping endpoint"""
    print("ğŸ”„ Triggering event scraping from Seniors Kingston website...")
    print("=" * 60)
    
    # Use local backend if running locally, otherwise use deployed URL
    backend_url = "http://localhost:8000"
    
    try:
        response = requests.post(f"{backend_url}/api/scrape-events", timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            print(f"\nâœ… Scraping completed!")
            print(f"   ğŸ“Š Total events: {result.get('events_count', 0)}")
            print(f"   â• Added: {result.get('added', 0)} new events")
            print(f"   ğŸ”„ Updated: {result.get('updated', 0)} existing events")
            print(f"   ğŸš« Skipped: {result.get('skipped', 0)} duplicates")
            print(f"\n   Message: {result.get('message', 'N/A')}")
            
            if result.get('updated_details'):
                print(f"\n   Updated events:")
                for detail in result.get('updated_details', [])[:5]:
                    print(f"      {detail}")
            
            return True
        else:
            print(f"âŒ Scraping failed: {response.status_code}")
            print(f"   Response: {response.text}")
            return False
            
    except requests.exceptions.ConnectionError:
        print(f"âŒ Could not connect to backend at {backend_url}")
        print("   Make sure the backend is running!")
        return False
    except Exception as e:
        print(f"âŒ Error: {e}")
        return False

if __name__ == "__main__":
    trigger_scraping()

